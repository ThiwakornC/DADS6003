{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThiwakornC/DADS6003/blob/main/DADS6003_6620422034_HW2(Lab3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "op-hHEW5p3_Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHMj49QGJONQ"
      },
      "source": [
        "Exercise1 <br>\n",
        "1.1 Change a number of features to 4 (x1,x2,x3,x4) </br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3LSmwS-JONU"
      },
      "source": [
        "# 1.1 Stochastic GD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[0,1],[2,6],[3,8],[4,10]])\n",
        "y = np.array([1,1,4])\n",
        "x_b = np.concatenate([np.ones((len(x),1)),x],axis=1)\n",
        "theta = np.random.random((x_b.shape[1],1))\n",
        "N = x.shape[0]\n",
        "\n",
        "def cost_function(theta, x_b, y, N):\n",
        "  y_hat = x_b.dot(theta)\n",
        "  c = np.sum(np.square(y_hat - y))\n",
        "  return c\n",
        "def gradient(x_b,y,theta,N):\n",
        "    y_hat = x_b.dot(theta)\n",
        "    grad = x_b.T.dot(y_hat - y)\n",
        "    return grad\n",
        "def stochasti_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=1,N=3,ep = 0.001):\n",
        "    J = cost_function(theta, x_b, y, N)\n",
        "    print(\"First J = \",J)\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    while not converged:\n",
        "      round = 1\n",
        "      #print(f'theta เริ่มต้น\\n {theta}')\n",
        "      #print('------------------------------------------------------------------')\n",
        "      for i in range (max_iter):\n",
        "        rand_index = np.random.randint(0,N)\n",
        "        #print(f'Round = {round}')\n",
        "        #print(f'Row {rand_index+1}')\n",
        "        xi = np.array(x_b[rand_index]).reshape(1,-1)\n",
        "        yi = np.array(y[rand_index]).reshape(1,-1)\n",
        "        #print(xi , yi)\n",
        "        grad = gradient(xi,yi,theta,N)\n",
        "        #print(f'Grad = {round}')\n",
        "        #print(grad)\n",
        "        #print('------------------------------------------------------------------')\n",
        "        theta = theta - learning_rate * grad\n",
        "        #print(f'theta = {round}')\n",
        "        #print(theta)\n",
        "        #print('------------------------------------------------------------------')\n",
        "        round += 1\n",
        "        J2 = cost_function(theta, x_b, y, N)\n",
        "\n",
        "      if abs(J-J2) <= ep:\n",
        "        print(f'Converged, iterations: {iter} / {max_iter}')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "      J = J2   # update error s\n",
        "      iter += 1  # update iter\n",
        "\n",
        "      if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "    return theta\n",
        "theta = stochasti_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=1000,N=3,ep = 0.000000000001)\n",
        "print(theta)\n",
        "xtest = np.array([[5,11]])\n",
        "xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "y_p = xtest_b.dot(theta) #y_hat\n",
        "print(\"y predict = \", y_p)"
      ],
      "metadata": {
        "id": "ldaGAWG3YHwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7a75bb-331a-48b2-e3ac-2483199dc0d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First J =  228.88567335090062\n",
            "Converged, iterations: 892 / 1000\n",
            "116.9999999998401 116.9999999998393\n",
            "[[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDicYUpzJONV"
      },
      "source": [
        "# 1.2 Mini-batch GD (size = 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[0,1],[2,6],[3,8],[4,10]])\n",
        "y = np.array([1,1,4])\n",
        "x_b = np.concatenate([np.ones((len(x),1)),x],axis=1)\n",
        "theta = np.random.random((x_b.shape[1],1))\n",
        "N = x.shape[0]\n",
        "batch_size = 2\n",
        "def cost_function_M(theta_M, x_b, y, N):\n",
        "  y_hat = x_b.dot(theta_M)\n",
        "  c = (1/batch_size)*np.sum(np.square(y_hat - y))\n",
        "  return c\n",
        "def gradient_b(X_batch,y_batch,theta_M,N):\n",
        "  y_hat = X_batch.dot(theta_M)\n",
        "  grad = (1/batch_size)*X_batch.T.dot(y_hat - y_batch)\n",
        "  return grad\n",
        "\n",
        "def MiniB_gradient_descent(x_b,y,theta_M,learning_rate=0.01,max_iter=10,N=3,ep = 0.000000000001):\n",
        "  J = cost_function_M(theta_M, x_b, y, N)\n",
        "  print(\"First J = \",J)\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  while not converged:\n",
        "    round = 1\n",
        "    for epoch in range(max_iter):\n",
        "      indices = np.random.permutation(N)\n",
        "      X_shuffled = x_b[indices]\n",
        "      y_shuffled = y[indices].reshape(-1, 1)\n",
        "      num_full_batches = N // batch_size\n",
        "      #print(num_full_batches)\n",
        "      for i in range(0, num_full_batches * batch_size, batch_size):\n",
        "        X_batch = X_shuffled[i:i+batch_size]\n",
        "        y_batch = y_shuffled[i:i+batch_size]\n",
        "        #print(f'round = {round}')\n",
        "        #print(X_shuffled)\n",
        "        #print(y_shuffled)\n",
        "        #print(f'\\nPrint x_batch \\n {X_batch}')\n",
        "        #print(y_batch)\n",
        "        grad = gradient_b(X_batch,y_batch,theta_M,N)\n",
        "        theta_M = theta_M - learning_rate * grad\n",
        "        round += 1\n",
        "        J2 = cost_function_M(theta_M, x_b, y, N)\n",
        "\n",
        "        #print(theta_M)\n",
        "        #print(grad)\n",
        "        #print(J,J2)\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(f'Converged, iterations: {iter} / {max_iter}')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    iter += 1  # update iter\n",
        "\n",
        "    if iter == max_iter:\n",
        "      print('       Max iterations exceeded!')\n",
        "      print(J,J2)\n",
        "      converged = True\n",
        "\n",
        "  return theta_M\n",
        "\n",
        "theta_M = MiniB_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=100000,N=3,ep = 0.000000000001)\n",
        "print(theta_M)\n",
        "xtest = np.array([[5,11]])\n",
        "xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "y_p = xtest_b.dot(theta_M)\n",
        "print(\"y predict = \", y_p)"
      ],
      "metadata": {
        "id": "__MMoQIfYKQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87a3622-3339-483e-e3e4-47ae461d482b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First J =  79.95386541722067\n",
            "Converged, iterations: 10 / 100000\n",
            "58.49999999999399 58.49999999999495\n",
            "[[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[16.]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}